{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b2f1fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import functional as F \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ed0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ElayerAvg(layer):\n",
    "    filters=torch.tensor([[0,1,0],[1,0,1],[0,1,0]]).float().to('cuda').reshape(1,1,3,3)\n",
    "    layerF=layer.float().to('cuda')\n",
    "    layerShape=layerF.shape\n",
    "    layerF=layerF.reshape(1,1,*layerShape)\n",
    "    elayer=F.conv2d(layerF,filters,padding=0)\n",
    "    layerF=layerF.reshape(*layerShape)\n",
    "    elayer=elayer.squeeze(0).squeeze(0)\n",
    "    return -(elayer*layerF[1:-1,1:-1]).mean()\n",
    "def ensure_divisibility(numerator, denominator):\n",
    "    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n",
    "    assert numerator % denominator == 0, '{} is not divisible by {}'.format(\n",
    "        numerator, denominator)\n",
    "\n",
    "\n",
    "def divide(numerator, denominator):\n",
    "    \"\"\"Ensure that numerator is divisible by the denominator and return\n",
    "    the division value.\"\"\"\n",
    "    ensure_divisibility(numerator, denominator)\n",
    "    return numerator // denominator\n",
    "def split_tensor_along_last_dim(tensor, num_partitions,\n",
    "                                contiguous_split_chunks=False):\n",
    "    \"\"\"Split a tensor along its last dimension.\n",
    "    Arguments:\n",
    "        tensor: input tensor.\n",
    "        num_partitions: number of partitions to split the tensor\n",
    "        contiguous_split_chunks: If True, make each chunk contiguous\n",
    "                                 in memory.\n",
    "    \"\"\"\n",
    "    # Get the size and dimension.\n",
    "    last_dim = tensor.dim() - 1\n",
    "    last_dim_size = divide(tensor.size()[last_dim], num_partitions) # 得到每个切分的size\n",
    "    # Split.\n",
    "    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim) # 对张量进行切分\n",
    "    # Note: torch.split does not create contiguous tensors by default.\n",
    "    if contiguous_split_chunks:\n",
    "        return tuple(chunk.contiguous() for chunk in tensor_list)\n",
    "\n",
    "    return tensor_list\n",
    "def calculateEnergyMat(model,n_heads,weightName):\n",
    "    Energy_Mat = []\n",
    "    for i in range(n_heads):\n",
    "        cur_layer_W_q = model[\"layers.{}.attention.{}.weight\".format(str(i),weightName)]\n",
    "        cur_heads = split_tensor_along_last_dim(cur_layer_W_q, n_heads)\n",
    "\n",
    "        cur_energy = map(ElayerAvg ,list(cur_heads))\n",
    "        Energy_Mat.append(list(cur_energy))\n",
    "    return torch.tensor(Energy_Mat)\n",
    "\n",
    "def Elayer(layer):\n",
    "    filters=torch.tensor([[0,1,0],[1,0,1],[0,1,0]]).float().to('cuda').reshape(1,1,3,3)\n",
    "    layerF=layer.float().to('cuda')\n",
    "    layerShape=layerF.shape\n",
    "    layerF=layerF.reshape(1,1,*layerShape)\n",
    "    elayer=F.conv2d(layerF,filters,padding=0)\n",
    "    layerF=layerF.reshape(*layerShape)\n",
    "    elayer=elayer.squeeze(0).squeeze(0)\n",
    "    return -(elayer*layerF[1:-1,1:-1])\n",
    "\n",
    "def avg_pool2attention(single_head, kernel_size):\n",
    "    layershape = single_head.shape\n",
    "    single_head=single_head.float().to('cuda')\n",
    "    single_head=single_head.reshape(1,1,*layershape)\n",
    "    out = F.avg_pool2d(single_head, kernel_size, ceil_mode=True)\n",
    "    out=out.squeeze(0).squeeze(0)\n",
    "    return out\n",
    "\n",
    "    def calculateEnergyMat_65B(model,n_layers,n_heads,weightName):\n",
    "        Energy_Mat = []\n",
    "        for i in range(n_layers):\n",
    "            cur_layer_W_q = model[\"layers.{}.attention.{}.weight\".format(str(i),weightName)]\n",
    "            cur_heads = split_tensor_along_last_dim(cur_layer_W_q, n_heads)\n",
    "            cur_energy = map(ElayerAvg ,list(cur_heads))\n",
    "            Energy_Mat.append(list(cur_energy))\n",
    "        return torch.tensor(Energy_Mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8f966c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 128])\n",
      "torch.Size([1022, 126])\n",
      "torch.Size([79, 63])\n"
     ]
    }
   ],
   "source": [
    "path='/data/shhliu19/wonder/llama/65B'\n",
    "model=torch.load(path+'/consolidated.00.pth')\n",
    "kernel_size=(13,2)\n",
    "cur_layer_W_q = model[\"layers.20.attention.wq.weight\"]\n",
    "with open(path+'/params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "n_heads=params['n_heads']\n",
    "cur_heads = split_tensor_along_last_dim(cur_layer_W_q, n_heads)\n",
    "print(list(cur_heads)[0].shape)\n",
    "Single_head_Energy_map = Elayer(list(cur_heads)[0])\n",
    "print(Single_head_Energy_map.shape)\n",
    "out = avg_pool2attention(Single_head_Energy_map, kernel_size)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ba963",
   "metadata": {},
   "source": [
    "#### 矩阵行翻转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c53bf95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([79, 63])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.flip(out, [0])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a76bce",
   "metadata": {},
   "source": [
    "### SSIM结构相似性比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5500c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ssim(img1, img2):  \n",
    "    img1, img2 = np.array(img1), np.array(img2)\n",
    "    # 此处因为转换为灰度值之后的图像范围是0-255，所以data_range为255，如果转化为浮点数，且是0-1的范围，则data_range应为1\n",
    "    ssim_score = ssim(img1, img2, data_range=255)\n",
    "    return ssim_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d98652",
   "metadata": {},
   "source": [
    "### KL散度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6017c",
   "metadata": {},
   "source": [
    "#### pooling q矩阵 vs energy map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34255597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'energy_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m kl \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mkl_div(out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlog()\u001b[38;5;241m.\u001b[39mcpu(), \u001b[43menergy_map\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu(), reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m kl\n",
      "\u001b[0;31mNameError\u001b[0m: name 'energy_map' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "kl = F.kl_div(out.view(-1).softmax(dim=-1).log().cpu(), energy_map[:-1,:-1].contiguous().view(-1).softmax(dim=-1).cpu(), reduction='sum')\n",
    "kl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa9e58",
   "metadata": {},
   "source": [
    "#### pooling q矩阵 vs self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d33190",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = F.kl_div(out.view(-1).softmax(dim=-1).log().cpu(), out.view(-1).softmax(dim=-1).cpu(), reduction='sum')\n",
    "kl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f31d39",
   "metadata": {},
   "source": [
    "#### random matrix vs energy map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bb58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = F.kl_div(torch.randn(79,64).view(-1).softmax(dim=-1).log().cpu(), energy_map[:-1,:].view(-1).softmax(dim=-1).cpu(), reduction='sum')\n",
    "kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = F.kl_div(energy_map[:-1,:].view(-1).softmax(dim=-1).log().cpu(), torch.randn(79,64).view(-1).softmax(dim=-1).cpu(), reduction='sum')\n",
    "kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(energy_map[:-1,:-1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(out.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48592c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-8.5075e-07)\n",
      "tensor(-7.4094e-07)\n",
      "tensor(-1.6282e-06)\n",
      "tensor(1.6820e-06)\n",
      "tensor(-1.2859e-06)\n",
      "tensor(2.1419e-06)\n",
      "tensor(-1.8161e-06)\n"
     ]
    }
   ],
   "source": [
    "kernel_size = (13, 2)\n",
    "with open(path+'/params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "n_heads=params['n_heads']\n",
    "n_layers=params['n_layers']\n",
    "\n",
    "for i in range(8):\n",
    "    model=torch.load(path+'/consolidated.0{}.pth'.format(i))\n",
    "    energy_map = calculateEnergyMat_65B(model,n_layers,n_heads,\"wq\")\n",
    "    cur_layer_W_q = model[\"layers.0.attention.wq.weight\"]\n",
    "    cur_heads = split_tensor_along_last_dim(cur_layer_W_q, n_heads)\n",
    "    Single_head_Energy_map = Elayer(list(cur_heads)[0])\n",
    "    out = avg_pool2attention(Single_head_Energy_map, kernel_size)\n",
    "    kl = F.kl_div(out.view(-1).softmax(dim=-1).log().cpu(), energy_map[:-1,:-1].contiguous().view(-1).softmax(dim=-1).cpu(), reduction='sum')\n",
    "    print(kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8df970",
   "metadata": {},
   "source": [
    "#### energy map & out 拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e342c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.stack([out.view(-1).cpu(), out.view(-1).cpu()*2],axis = 0)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5cf3c",
   "metadata": {},
   "source": [
    "#### 65B kernel size & crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7794605",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/data/shhliu19/wonder/llama/65B'\n",
    "\n",
    "kernel_size = (13, 2)\n",
    "with open(path+'/params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "n_heads=params['n_heads']\n",
    "n_layers=params['n_layers']\n",
    "model=torch.load(path+'/consolidated.00.pth'.format(i))\n",
    "energy_map = calculateEnergyMat_65B(model,n_layers,n_heads,\"wv\")\n",
    "cur_layer_W_q = model[\"layers.0.attention.wv.weight\"]\n",
    "cur_heads = split_tensor_along_last_dim(cur_layer_W_q, n_heads)\n",
    "print(list(cur_heads)[0].shape)\n",
    "Single_head_Energy_map = Elayer(list(cur_heads)[0])\n",
    "print(Single_head_Energy_map.shape)\n",
    "out = avg_pool2attention(Single_head_Energy_map, kernel_size)\n",
    "print(out.shape)\n",
    "print(energy_map.shape)\n",
    "kl = F.kl_div(out.view(-1).softmax(dim=-1).log().cpu(), energy_map[:-1,:-1].contiguous().view(-1).softmax(dim=-1).cpu(), reduction='sum')\n",
    "print(kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db16fc1",
   "metadata": {},
   "source": [
    "#### 13B kernel size & crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8003da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/data/shhliu19/wonder/llama/13B'\n",
    "kernel_size = (64,3) #13B\n",
    "with open(path+'/params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "n_heads=params['n_heads']\n",
    "n_layers=params['n_layers']\n",
    "model=torch.load(path+'/consolidated.00.pth'.format(i))\n",
    "energy_map = calculateEnergyMat_65B(model,n_layers,n_heads,\"wv\")\n",
    "cur_layer_W_q = model[\"layers.0.attention.wv.weight\"]\n",
    "cur_heads = split_tensor_along_last_dim(cur_layer_W_q, n_heads)\n",
    "print(list(cur_heads)[0].shape)\n",
    "Single_head_Energy_map = Elayer(list(cur_heads)[0])\n",
    "print(Single_head_Energy_map.shape)\n",
    "out = avg_pool2attention(Single_head_Energy_map, kernel_size)\n",
    "print(out.shape)\n",
    "print(energy_map.shape)\n",
    "kl = F.kl_div(out[:,:-2].contiguous().view(-1).softmax(dim=-1).log().cpu(), energy_map.view(-1).softmax(dim=-1).cpu(), reduction='sum')\n",
    "print(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/data/shhliu19/wonder/llama/65B'\n",
    "\n",
    "kernel_size = (13, 2)\n",
    "with open(path+'/params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "n_heads=params['n_heads']\n",
    "n_layers=params['n_layers']\n",
    "model=torch.load(path+'/consolidated.00.pth.fake'.format(i))\n",
    "energy_map = calculateEnergyMat_65B(model,n_layers,n_heads,\"wv\")\n",
    "cur_layer_W_q = model[\"layers.0.attention.wv.weight\"]\n",
    "cur_heads = split_tensor_along_last_dim(cur_layer_W_q, n_heads)\n",
    "print(list(cur_heads)[0].shape)\n",
    "Single_head_Energy_map = Elayer(list(cur_heads)[0])\n",
    "print(Single_head_Energy_map.shape)\n",
    "out = avg_pool2attention(Single_head_Energy_map, kernel_size)\n",
    "print(out.shape)\n",
    "print(energy_map.shape)\n",
    "kl = F.kl_div(out.view(-1).softmax(dim=-1).log().cpu(), energy_map[:-1,:-1].contiguous().view(-1).softmax(dim=-1).cpu(), reduction='sum')\n",
    "print(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b3b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn_like()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
